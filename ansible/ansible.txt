WHAT IS ANSIBLE ?
Ansible is an open source IT automation engine that automates 
- provisioning resources like terraform
- configuration management
- application deployment
- network automation

and many other IT processes. It is free to use, and the project benefits from the experience and intelligence of its thousands of contributors.

How Ansible works ?
  Ansible is agentless in nature, which means you don't need install any software on the manage nodes.

CONTROL NODE, MANAGE NODES:
Control Node: 
   A machine where we install ansible is control node.
Manage Node: 
   Using control we manage multiple machines or vm for configuring softwares and packages are called mange nodes.

For Example:
   - if we want to install java on multiple machines that are redhat, windows, ubuntu based. we can write ansible script in yaml in control node and can install the java on th manage nodes using the ansible cmds in control node. 
   - no need to install any agents on the manage nodes, but they just need to have python on both control and manage nodes.
   - yaml script you written on control node would be converted by ansible into python and execute that on manage nodes.
   - control node would be connected to manage nodes suing ssh for linux machines and winrm for windows. control node execute scripts either in parallelly on all nodes at atime or one by one.
   - we can also give instructions in ansible adhoc cmds which would executed by controlnode on mange node to configure

For automating Linux and Windows, Ansible connects to managed nodes and pushes out small programs—called Ansible modules—to them. These programs are written to be resource models of the desired state of the system. Ansible then executes these modules (over SSH by default), and removes them when finished. These modules are designed to be idempotent when possible, so that they only make changes to a system when necessary.

For automating network devices and other IT appliances where modules cannot be executed, Ansible runs on the control node. Since Ansible is agentless, it can still communicate with devices without requiring an application or service to be installed on the managed node.

##########################################################

Comparison with Shell Scripting
- Shell Scripting works only for Linux.
- Becomes complex and less readable(for non-experts) as the script size goes high.
- Idempotence and predictability

When the system is in the state your playbook describes Ansible does not change anything, even if the playbook runs multiple times.

for example, run the below shell script twice and you will notice the script will fail. Which means shell scripting is not idempotent in nature.

---
#/bin/bash

set -e 

mkdir test-demo
echo "hi"
---
##########################################################

PASSWORDLESS AUTHENTICATION:
  passwordless auth nothing but doing connection/auth between control and manage nodes without password, except we required the password or pemkey for the initial connection. So that control node can run the ansible scripts or adhoc cmds on the manage node without needing authentication


Normal Auth : source will be having private key and target will be having public key, we establish ssh connection between them.


# EC2 Instances

Using Public Key
------
ssh-copy-id -f "-o IdentityFile <PATH TO PEM FILE>" ubuntu@<INSTANCE-PUBLIC-IP>
------

After initial auth we can directly connect to managed node using below
------
ssh ubuntu@<INSTANCE-PUBLIC-IP>
------
- ssh-copy-id: This is the command used to copy your public key to a remote machine.
- -f: This flag forces the copying of keys, which can be useful if you have keys already set up and want to overwrite them.
- "-o IdentityFile <PATH TO PEM FILE>": This option specifies the identity file (private key) to use for the connection. The -o flag passes this option to the underlying ssh command.
- ubuntu@<INSTANCE-IP>: This is the username (ubuntu) and the IP address of the remote server you want to access.

Using Password 
- Go to the file `/etc/ssh/sshd_config.d/60-cloudimg-settings.conf`
- Update `PasswordAuthentication yes`
- Restart SSH -> `sudo systemctl restart ssh`

#########################################################

INVENTORY:
 Ansible inventory file is a fundamental component of Ansible that defines the hosts (remote systems) that you want to manage and the groups those hosts belong to. 
 The inventory file can be static (a simple text file) or dynamic (generated by a script). It provides Ansible with the information about the remote nodes to communicate with during its operations.
 we can group the managed nodes based on the usage i.e webservers, dbs, appservers

Static Inventory
A static inventory file is typically a plain text file (usually named hosts or inventory) and is structured in INI or YAML format. Here are examples of both formats:

#INI Format
---
# inventory file: hosts

[webservers]
web1.example.com
web2.example.com

[dbservers]
db1.example.com
db2.example.com

[all:vars]
ansible_user=admin
ansible_ssh_private_key_file=/path/to/key
---

#YAML
---
# inventory file: hosts.yaml

all:
  vars:
    ansible_user: admin
    ansible_ssh_private_key_file: /path/to/key
  children:
    webservers:
      hosts:
        web1.example.com:
        web2.example.com:
    dbservers:
      hosts:
        db1.example.com:
        db2.example.com:
---

Dynamic Inventory:
A dynamic inventory is generated by a script or plugin and can be used for environments where hosts are constantly changing (e.g., cloud environments). The script or plugin fetches the list of hosts from a source like AWS, GCP, or any other dynamic source.

COMMAND
---
ansible-playbook -i inventory <Adhoc command or Playbook.yml>
---
when to use adhoc cmds
  - installing apache server or other softwares
  - copying files to server
  - restarting the servers
  - simple tasks
  
when to use playbooks(yaml)  
  - tasks involves multiple steps or complex tasks we have to use this

COMMANDS STRUCTURE:
  ansible -i inventory -m [module] -a [argument] [server]
  ansible -i inventory -m shell -a "sudo ls etc" all
  ansible -i inventory -m shell -a "sudo ls etc" web1.example.com
  ansible -i inventory -m ping all
  
###########################################

MODULES:
  Ansible modules are discrete units of code that perform specific tasks on managed nodes, executing actions like installing software, managing users, or configuring network devices.   
  
CATEGORY 	PURPOSE	                     COMMON EXAMPLES
System	    OS-level tasks	              user, group, service, hostname
Packaging	Software installation      	  apt, yum, dnf, pip
Files	    File manipulation	          copy, file, template, synchronize
Cloud	    Infrastructure provisioning	  ec2_instance, gcp_compute_instance
Commands	Running arbitrary scripts	  command, shell, raw  

###########################################

# Ansible Concepts: Playbook, Play, Modules, Tasks, and Collections

Playbook
A **Playbook** is a YAML file that defines a series of actions to be executed on managed nodes. It contains one or more "plays" that map groups of hosts to roles.

PLAY: 
 a play will have a name, host, remote_user and tasks
 tasks will be having list of modules, usimg these modules we execute tasks or cmds on hosts or manage nodes
   ansible.builtin.yum = using ansible builtin module yum we install httpd on manage node

---
- name: Update web servers
  hosts: webservers
  remote_user: root

  tasks:
  - name: Ensure apache is at the latest version
    ansible.builtin.yum:
      name: httpd
      state: latest 
---

Example:
---
---
- name: Update web servers
  hosts: webservers
  remote_user: root

  tasks:
  - name: Ensure apache is at the latest version
    ansible.builtin.yum:
      name: httpd
      state: latest

  - name: Write the apache config file
    ansible.builtin.template:
      src: /srv/httpd.j2
      dest: /etc/httpd.conf

- name: Update db servers
  hosts: databases
  remote_user: root

  tasks:
  - name: Ensure postgresql is at the latest version
    ansible.builtin.yum:
      name: postgresql
      state: latest

  - name: Ensure that postgresql is started
    ansible.builtin.service:
      name: postgresql
      state: started
---

PLAY:
 A Play is a single, complete execution unit within a playbook. It specifies which hosts to target and what tasks to execute on those hosts. Plays are used to group related tasks and execute them in a specific order.

---
- name: Install and configure Nginx
  hosts: webservers
  tasks:
    - name: Install Nginx
      apt:
        name: nginx
        state: present
---

MODULES:
 Modules are the building blocks of Ansible tasks. They are small programs that perform a specific action on a managed node, such as installing a package, copying a file, or managing services.
Example

The apt module used in a task to install a package:

---
- name: Install Nginx
  apt:
    name: nginx
    state: present
---

TASKS:
 Tasks are individual actions within a play that use modules to perform operations on managed nodes. Each task is executed in order and can include conditionals, loops, and handlers.
      
---
- name: Install Nginx
  apt:
    name: nginx
    state: present

- name: Start Nginx service
  service:
    name: nginx
    state: started
---

EXAMPLE:
---
- hosts: all
  become: true
  tasks:
    - name: Install apache httpd
      ansible.builtin.apt:
        name: apache2
        state: present 
        update_cache: yes
    - name: Copy file with owner and permissions
      ansible.builtin.copy:
        src: index.html
        dest: /var/www/html
        owner: root
        group: root
        mode: '0644'
		
----------------------------------
ansible-playbook -i inventory Playbook.yml

state: present -> install
state: absent -> uninstall
hosts: all -> on all managed nodes. we can use groups like db, webserver etc. we can also use single host.

- after executing the playbook task would execute and first task Gathering Facts, it check whether it can connect to host or not.

--------------------------
COLLECTIONS
Collections are a distribution format for Ansible content. They bundle together multiple roles, modules, plugins, and other Ansible artifacts. Collections make it easier to share and reuse Ansible content.
Example

A collection structure might look like this:

---
my_collection/
├── roles/
│   └── my_role/
│       └── tasks/
│           └── main.yml
├── plugins/
│   └── modules/
│       └── my_module.py
└── README.md
---

Using a Collection

---
- name: Use a custom module from a collection
  community.general.my_module:
    option: value
---
-----------------------

###########################################

ANSIBLE ROLES:
  An Ansible role is a reusable, self-contained unit of automation that is used to organize and manage tasks, variables, files, templates, and handlers in a structured way. 

  Roles help to encapsulate and modularize the logic and configuration needed to manage a particular system or application component. 

  This modular approach promotes reusability, maintainability, and consistency across different playbooks and environments.

Key Components of an Ansible Role
Tasks:
 The main list of actions that the role performs.

Handlers:
 Tasks that are triggered by changes in other tasks, typically used for actions like restarting services.

Files:
 Static files that need to be transferred to managed hosts.

Templates:
 Jinja2 templates that can be rendered and transferred to managed hosts.

Vars:
 Variables that are used within the role.

Defaults:
Default variables for the role, which can be overridden.

Meta:
 Metadata about the role, including dependencies on other roles.

Library:
 Custom modules or plugins used within the role.

Module_defaults:
 Default module parameters for the role.

Lookup_plugins:
 Custom lookup plugins for the role.

Directory Structure of an Ansible Role

An Ansible role follows a specific directory structure:

---
<role_name>/
  ├── defaults/
  │   └── main.yml
  ├── files/
  ├── handlers/
  │   └── main.yml
  ├── meta/
  │   └── main.yml
  ├── tasks/
  │   └── main.yml
  ├── templates/
  ├── vars/
      └── main.yml
---

- Just like we can build docker images and push them docker hub, we can create roles and upload them to ansible galaxy or git hub to share across teams or outside world.

EXECUTION:
1. ansible-galaxy role init <demo>
3. then the dolder will created under demo name
4. then we have to copy all configurations like tasks, vars, meta, handlers, files inside of main.yml's of individual folders.
5. then in playbook.yml remove all the configurations like tasks, vars, meta, handlers, files and add "role:" so that the all of them would be mapped into playbook.yml and will executed. Also ansible knows which role to execute when you run playbook.
    roles:
     - role-name
6. mostly demo(role) and playbook.yml present at same location.
7. now execute playbook.yml with "ansible-playbook -i inventory Playbook.yml"
8. now all the modules, vars, tasks, files etc would get fetched from demo(role) while execution on hosts.
9. thus roles helps in modularizing big playbooks into individual files.
---
ls -ltr 
  playbook.yml  demo  inventory
---  

- roles will be pushed and pulled to galaxy.ansible.com like central repo for all roles. we can also use others roles, below are the various cmds.
ansible-galaxy role init <name>	    -> Creates a skeleton directory for a new role.
ansible-galaxy role install <name>	-> Downloads and installs a role from Galaxy.
ansible-galaxy role list	        -> Displays all currently installed roles and their versions.
ansible-galaxy role search <term>	-> Searches the Galaxy database for specific roles.
ansible-galaxy role remove <name>	-> Deletes an installed role from your local path.

- role will intialluy pushed to github and later we have import the roles from gitrepo.
- tils better to signup galaxy.ansible.com with github account, so we can easily integrate.

ansible-galaxy import <github-user-name> <repo-name> --token <token>
  
  toke = apitoken will be available from collections in galaxy.ansible.com to auth
  - instead of pass token directly in cmd we can also store in another file and map it.
  
######################################################

COLLECTIONS:
  - collections nothing but 3rd party modules like aws, azure etc that you want to interact or create and manage the resources.
  - just like providers, collections in ansible used to talk with aws api to create resource, for that we havt to install aws-modules on control node.
  - to run aws-modules has pre-requisite boto3 to talk with the aws api.
  - collections mainly used for infra provisioning and network automation as it requires to talk with 3rd party api we require secrete key and accesskey which will be stored in vault and passed as vars in playbook or roles.
  - in terms of built-in modules these get executed on the manage nodes, here the collections get executed in control-node as these doesnt need manage nodes we are just creating the infra using control node.


Setup EC2 Collection and Authentication

Install boto3
---
pip install boto3
---

Install AWS Collection
---
ansible-galaxy collection install amazon.aws
---

SETUP VAULT:
  - to establish connection between the control node and aws to create resource we require aws access and secret keys.
  - we have to ansible vault and store the aws access and secret keys as bewlow shown way.
1. Create a password for vault
---
openssl rand -base64 2048 > vault.pass
---

2. Add your AWS credentials using the below vault command
---
ansible-vault create group_vars/all/pass.yml --vault-password-file vault.pass
---  
palybook.yml
--- 
- hosts: localhost
  connection: local
  vars:
    region: us-east-1 #standard way of declaring variable in playbook
  tasks:
  - name: start an instance with a public IP address
    amazon.aws.ec2_instance:
      name: "ansible-instance"
      # key_name: "prod-ssh-key"
      # vpc_subnet_id: subnet-013744e41e8088axx
      instance_type: t2.micro
      security_group: default
      region: {{region}}
      aws_access_key: "{{ec2_access_key}}"  # From vault as defined
      aws_secret_key: "{{ec2_secret_key}}"  # From vault as defined      
      network:
        assign_public_ip: true
      image_id: ami-04b70fa74e45c3917
      tags:
        Environment: Testing

-> for above ansible-playbook playbook.yaml is command as it is get executed on local host.
-> "{{  }}" is Jinja2 syntax to reference variables in Ansible.
-> "{{ec2_access_key}}"  "{{ec2_secret_key}}" are variables stored in Ansible Vault for security.
-> we can declare the variables in the default vars file or vars file specific to the playbook.

-> we have to use roles (not mandatory) to use default vars file or vars file specific to the playbook and map the tasks to the role.
playbook.yaml
---
- hosts: localhost
  connection: local
  roles:
    - ec2_instance_role
ansible-playbook role init ec2_instance_role

DECLARING VARIABLES IN DEFAULT VARS FILE:
----
default/main.yml
    type: t2.micro
-----

PASSING VARIABLES FROM VARS FILE SPECIFIC TO THE PLAYBOOK IN TASKS:
tasks/main.yml
------
  - name: start an instance with a public IP address
    amazon.aws.ec2_instance:
      name: "ansible-instance"
      instance_type: "{{type}}"
      security_group: default
      region: us-east-1
      aws_access_key: "{{ec2_access_key}}"  # From vault as defined
      aws_secret_key: "{{ec2_secret_key}}"  # From vault as defined      
      network:
        assign_public_ip: true
      image_id: ami-04b70fa74e45c3917
      tags:
        Environment: Testing  
------

-> Declaring variable in vars file specific to the playbook. these are higher priority than default vars file.
-> variables in vars file specific to the playbook override the default vars file.
DECLARING VARIABLE IN VARS FILE:
vars/main.yml
------
    type: t2.medium
------
ansible-playbook playbook.yaml 
   here -i not needed as localhost is defined in playbook.
   
-> we can also pass variables at command line using --extra-vars or -e option.
-> this has highest priority and overrides both default vars file and vars file specific to the playbook.
     ansible-playbook playbook.yaml -e "type=t2.large"

-> we can also decalare variables in groups_vars or host_vars directories which has higher priority than default vars file but lower than vars file specific to the playbook.
-> vars uploaded in group folder can be used by all hosts
-> these will be used by the hosts defined in inventory file.

######################## Tasks #####################

TASK 1:
Create three(3) EC2 instances on AWS using Ansible loops
- 2 Instances with Ubuntu Distribution
- 1 Instance with Centos Distribution
Hint: Use `connection: local` on Ansible Control node.

LOOPS: 
 - loops in ansible to create multiple EC2 instances with different configurations.
 - they will use different AMI IDs and names for each instance.
 - not only that, but also using vault variables for sensitive information like AWS access keys.
 - note: Make sure to update the AMI IDs to ones that are valid in your AWS account and region.
---
- hosts: localhost
  connection: local

  tasks:
  - name: Create EC2 instances
    amazon.aws.ec2_instance:
      name: "{{ item.name }}"
      key_name: "abhi-aws-keypair"
      instance_type: t2.micro
      security_group: default
      region: ap-south-1
      aws_access_key: "{{ec2_access_key}}"  # From vault as defined
      aws_secret_key: "{{ec2_secret_key}}"  # From vault as defined      
      network:
        assign_public_ip: true
      image_id: "{{ item.image }}"
      tags:
        environment: "{{ item.name }}"
    loop:
      - { image: "ami-0e1d06225679bc1c5", name: "manage-node-1" } # Update AMI ID according 
      - { image: "ami-0f58b397bc5c1f2e8", name: "manage-node-2" } # to your account
      - { image: "ami-0f58b397bc5c1f2e8", name: "manage-node-3" }

ANSIBLE COMMAND TO RUN THE PLAYBOOK:
-----
ansible-playbook ec2_create.yaml --ask-vault --vault-password-file vault.pass
-----
-> we have to generate the vault and store the secrets in yaml file insise vault, so that keys can be used in playbook.
   here, 'vault.pass' is a file that contains the access key and secret key for AWS in encrypted format.
pass.yaml (vault file):
----
ec2_secret_key:
ec2_access_key:
----

TASK 2:
Set up passwordless authentication between Ansible control node and newly created instances.
commands:
-----
ssh-copy-id -f "-o IdentityFile <PATH TO PEM FILE>" ubuntu@<INSTANCE-PUBLIC-IP>
ssh ubuntu@<INSTANCE-PUBLIC-IP>  # to verify passwordless login
-----

TASK 3:
  Automate the shutdown of Ubuntu Instances only using Ansible Conditionals
  Hint: Use `when` condition on ansible `gather_facts`

WHEN CONDITION: 
  This playbook demonstrates the use of 'when' condition to shutdown only Ubuntu (Debian family) EC2 instances.
  this below task would be executed on the manage nodes
---
- hosts: all
  become: true

  tasks:
    - name: Shutdown ubuntu instances only
      ansible.builtin.command: /sbin/shutdown -t now
      when:
       ansible_facts['os_family'] == "Debian"

COMMAND TO RUN THE PLAYBOOK:
-----
ansible-playbook -i inventory.ini ec2_shutdown.yaml --vault-password-file vault.pass
-----
-> here -i inventory.ini contains the list of EC2 instances created in the previous playbook and we will execute the shutdown playbook on those instances.

######################### Error Handling #################################

ERROR-HANDLING: 
 - by default ansible executes the task1 on all the hosts then continues to task2.
 - for example i have 3 tasks and 3 hosts, if task1 fails in host1 ansible will continue executing the task1 on host2 and host3, then it will continue to ignore task2 & task3 on host1 and execute task2 on host2 and host3 and so on.
 - now this can be changed using "ignore_errors: yes" which will ignore the error and continue executing the next tasks on the same host after completing the current task on all hosts. this called error handling in ansible.

---
- hosts: all
  become: true # to run tasks with sudo privileges

  tasks:
    - name: Install security updates
      ansible.builtin.apt:
        name: "{{ item }}"
        state: latest
      loop:
        - openssl
        - openssh
      ignore_errors: yes 
    - name: Check if docker is installed
      ansible.builtin.command: docker --version
      register: output
      ignore_errors: yes    
    - ansible.builtin.debug:
        var: output
    - name: Install docker
      ansible.builtin.apt:
        name: docker.io
        state: present
      when: output.failed

EXPLANATION:
  - Here task1 checks for openssh & openssl existence and installs the latest version. if it doesnt found any of them it will fails, to avoid that we used "ignore_errors: yes" so that ansible will continue executing the next tasks.
  - In task2 we are checking if docker is installed using "docker --version" command. also we are using "ignore_errors: yes" to avoid failure if docker is not installed and store the result in "output" variable using "register: output".
  - In task3 we are printing the output variable to see the result of the command. if the output shows failed from task2 then we can conclude that docker is not installed. then task3 will install docker using apt module.

-> "register: output" : will store the result of the command task. if the command fails, output.failed will be true. so the docker installation task will only run if the docker --version command fails. 

################# Security in Ansible ##################

1. Create a password for vault 
---
openssl rand -base64 2048 > vault.pass
---

2. now we can use vault password and encrypt the keys using below cmd. valut.pass will have a encrypted password which will encrypts the pass.yml 
----
ansible-vault create group_vars/all/pass.yml --vault-password-file vault.pass
----

3.the above cmd vill open vim add now add access and secretkey
----
ec2_secret_key:
ec2_access_key:
---

to decrypt
ansible-vault decrypt group_vars/all/pass.yml --vault-password-file vault.pass

to edit
ansible-vault edit group_vars/all/pass.yml --vault-password-file vault.pass

to encrypt already existing keys
ansible-vault encrypt group_vars/all/pass01.yml

4. we can also use without --vault-password-file vault.pass, but we have to set password and use that pass word always

Best Practices:
-> have a strong password and better to maintain each env(dev, test, prod) has their own password
-> store them in secrts manager, use iam permissions to restrict and encrypt using kms.

################### policy as code ######################

POLICY AS CODE:
  Policy as Code (PaC) in DevSecOps refers to the practice of defining and managing security policies through code. This approach enables automated, consistent, and scalable enforcement of security controls and compliance requirements across the software development lifecycle. 

EXAMPLE USE CASES:
Infrastructure Security:
Ensure that cloud resources (e.g., AWS S3 buckets, IAM roles) comply with security best practices.
Automatically remediate non-compliant resources.

Application Security:
Enforce secure coding practices and compliance checks during the build and deployment stages.
Prevent deployment of applications with known vulnerabilities.

Compliance and Governance:
Implement regulatory compliance requirements (e.g., GDPR, HIPAA) as code.
Continuously monitor and enforce compliance across the organization.

INSTALL AND SETUP ANSIBLE FOR IMPLEMENTING POLICY AS CODE ON AWS
1. Install boto3
```
pip install boto3
```

2. Install AWS Collection
```
ansible-galaxy collection install amazon.aws
```

SETUP VAULT 
note : no need to do if you use aws configure on local
1. Create a password for vault
```
openssl rand -base64 2048 > vault.pass
```

2. Add your AWS credentials using the below vault command
```
ansible-vault create group_vars/all/pass.yml --vault-password-file vault.pass


EXAMPLE: 
playbook.yaml
---
- name: Enforce s3 bucket versioning on AWS account
  hosts: localhost
  gather_facts: false

  tasks:
    - name: List S3 buckets in AWS account
      amazon.aws.s3_bucket_info:
      register: result
    
    - debug:
        var: result
    
    - name: Enable versioning on S3 bucket
      amazon.aws.s3_bucket:
        name: "{{ item.name }}" 
        versioning: yes
      loop: "{{ result.buckets }}" 

command:
----
ansible-playbook playbook.yaml
----

- here item.name refers to the name of each S3 bucket obtained from the previous task.
- result.buckets is a list of all S3 buckets in the AWS account.      
